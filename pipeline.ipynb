{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Road Desert Semantic Segmentation — SegFormer Pipeline\n",
    "\n",
    "Multiclass semantic segmentation using **SegFormer (MiT-B2)** encoder with HuggingFace Transformers.  \n",
    "Evolved from the DeepLabV3+ / ResNet50 baseline (~64% val mIoU) to leverage transformer global context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not already present (needed for SegFormer)\n",
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = \"dataset\"\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 40           # Increased from 30: transformers converge slower\n",
    "BATCH_SIZE = 4        # Same as DeepLabV3+ baseline — fits 8GB VRAM with AMP\n",
    "IMG_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# SegFormer-specific configuration\n",
    "# B2 pretrained on ADE20K (150 scene classes) — strong transfer for outdoor scenes.\n",
    "# The classifier head gets reinitialized for our 10 classes.\n",
    "# SEGFORMER_VARIANT = \"nvidia/segformer-b2-finetuned-ade-512-512\"\n",
    "# If OOM on your GPU, uncomment the B1 variant (~14M params vs ~27M for B2):\n",
    "SEGFORMER_VARIANT = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "\n",
    "# Learning rates — transformers need lower LR than CNNs to avoid\n",
    "# destabilizing pretrained self-attention weights.\n",
    "ENCODER_LR = 6e-5    # Gentle: preserve pretrained MiT representations\n",
    "DECODER_LR = 6e-4    # Faster: MLP decode head adapts to our classes\n",
    "WEIGHT_DECAY = 0.01  # Standard for transformers (vs 1e-4 for CNNs)\n",
    "WARMUP_EPOCHS = 3    # Linear warmup before cosine decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class ID Mapping\n",
    "\n",
    "Mask pixel values are non-contiguous IDs: `{100, 200, 300, 500, 550, 600, 700, 800, 7100, 10000}`.  \n",
    "We remap them to contiguous `[0–9]` via a lookup table for O(1) per-pixel mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original class IDs found in the masks -> contiguous labels [0-9]\n",
    "CLASS_IDS = [100, 200, 300, 500, 550, 600, 700, 800, 7100, 10000]\n",
    "NUM_CLASSES = len(CLASS_IDS)\n",
    "\n",
    "# Build a lookup table for fast remapping (max value is 10000, so table size is 10001)\n",
    "_REMAP_LUT = np.full(10001, 0, dtype=np.int64)  # default to 0 for any unexpected value\n",
    "for contiguous_label, original_id in enumerate(CLASS_IDS):\n",
    "    _REMAP_LUT[original_id] = contiguous_label\n",
    "\n",
    "\n",
    "def remap_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Remap raw mask pixel values to contiguous class indices [0, NUM_CLASSES-1].\"\"\"\n",
    "    return _REMAP_LUT[mask]\n",
    "\n",
    "\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(f\"ID mapping: {dict(zip(CLASS_IDS, range(NUM_CLASSES)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n",
    "\n",
    "Pairs images and masks by **sorted index** — filenames do NOT match between `Color_Images/` and `segmentation/`.  \n",
    "Masks are loaded as uint16 (values like 7100 and 10000 exceed uint8 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pairs images and masks by SORTED INDEX, not by filename.\n",
    "    This is required because filenames differ between the two folders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Sort both lists independently — alignment is by index\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), (\n",
    "            f\"Mismatch: {len(self.images)} images vs {len(self.masks)} masks\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image as RGB (OpenCV loads BGR by default)\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load mask as uint16 grayscale — critical for values > 255 like 7100, 10000\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # If mask was loaded as 3-channel, take first channel\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        # Remap to contiguous class IDs [0-9]\n",
    "        mask = remap_mask(mask).astype(np.int64)\n",
    "\n",
    "        # Apply augmentations — Albumentations treats mask as integer labels automatically\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]           # float32 tensor [3, H, W]\n",
    "            mask = augmented[\"mask\"]             # int64 tensor [H, W]\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss / segmentation losses\n",
    "        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation\n",
    "\n",
    "Adapted for SegFormer vs the DeepLabV3+ CNN baseline. Key changes explained inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size=512):\n",
    "    \"\"\"\n",
    "    Augmentation strategy adapted for SegFormer (transformer architecture).\n",
    "\n",
    "    vs. CNN (DeepLabV3+) pipeline, three principles guide the changes:\n",
    "\n",
    "    1. GEOMETRIC AUGMENTATIONS RETAINED at moderate intensity.\n",
    "       Transformers lack CNN's built-in translation equivariance.\n",
    "       They must learn spatial invariance from data, so geometric\n",
    "       diversity (shifts, scales, rotations) remains critical.\n",
    "\n",
    "    2. COLOR / NOISE AUGMENTATIONS REDUCED.\n",
    "       Transformers tokenize images into patches and embed them linearly.\n",
    "       Heavy color distortion destabilizes patch embeddings more than\n",
    "       conv features, especially early in fine-tuning. Self-attention\n",
    "       already generalizes texture better than CNNs, so heavy noise\n",
    "       provides diminishing returns.\n",
    "\n",
    "    3. ImageCompression REMOVED.\n",
    "       Broken API in albumentations v2 + minimal benefit for transformers.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # Geometric: kept similar to baseline (shift/scale/rotate).\n",
    "        # Slightly reduced rotate_limit (30->20) — less extreme rotations\n",
    "        # are more realistic for off-road camera views with stable horizon.\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, scale_limit=0.15, rotate_limit=20,\n",
    "            border_mode=cv2.BORDER_REFLECT_101, p=0.5,\n",
    "        ),\n",
    "        # Desert lighting: harsh shadows are a real domain trait. Kept.\n",
    "        A.RandomShadow(p=0.3),\n",
    "        # REDUCED from baseline (0.3/0.3 limits, p=0.6 -> 0.2/0.2, p=0.4).\n",
    "        # Protects pretrained patch embeddings from too much brightness shock.\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),\n",
    "        # REDUCED from baseline (0.2/0.2/0.2/0.1 at p=0.5 -> 0.1/0.1/0.1/0.05 at p=0.25).\n",
    "        # Mild jitter preserves embedding stability while still adding\n",
    "        # color variance for synthetic-to-real domain gap.\n",
    "        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.25),\n",
    "        # REDUCED overall probability (p=0.4 -> 0.2), removed ImageCompression.\n",
    "        # Transformers generalize texture through self-attention across patches,\n",
    "        # so heavy noise/blur just hurts convergence without useful regularization.\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=1.0),\n",
    "            A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "        ], p=0.2),\n",
    "        # ImageNet normalization — non-negotiable. SegFormer's MiT encoder\n",
    "        # was pretrained with these exact statistics.\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms(img_size=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SegmentationDataset(\n",
    "    image_dir=os.path.join(DATA_ROOT, \"train\", \"Color_Images\"),\n",
    "    mask_dir=os.path.join(DATA_ROOT, \"train\", \"segmentation\"),\n",
    "    transform=get_train_transforms(IMG_SIZE),\n",
    ")\n",
    "val_ds = SegmentationDataset(\n",
    "    image_dir=os.path.join(DATA_ROOT, \"val\", \"Color_Images\"),\n",
    "    mask_dir=os.path.join(DATA_ROOT, \"val\", \"segmentation\"),\n",
    "    transform=get_val_transforms(IMG_SIZE),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples | Val: {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "**SegFormer** replaces DeepLabV3+ as the segmentation architecture.\n",
    "\n",
    "**Why SegFormer may outperform DeepLabV3+/ResNet50 on this task:**\n",
    "\n",
    "| Aspect | DeepLabV3+ (CNN) | SegFormer (Transformer) |\n",
    "|--------|-----------------|------------------------|\n",
    "| Receptive field | Local (grows via stacking) | Global from layer 1 (self-attention) |\n",
    "| Large uniform regions | Struggles (sky, flat sand) | Strong (full-image context) |\n",
    "| Texture bias | High (synthetic data risk) | Lower (attends to structure/shape) |\n",
    "| Position encoding | Implicit via convolutions | None — position-free, scale-robust |\n",
    "| Decoder | Heavy ASPP module | Lightweight MLP (relies on richer encoder) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps HuggingFace SegformerForSemanticSegmentation to output logits\n",
    "    at full input resolution [B, NUM_CLASSES, H, W].\n",
    "\n",
    "    SegFormer natively outputs at H/4 x W/4. We bilinear-upsample so\n",
    "    the existing loss (DiceFocalLoss) and metric (MulticlassIoU) code\n",
    "    works without any modification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            pretrained_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True,  # reinit classifier for our 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        logits = outputs.logits  # [B, num_classes, H/4, W/4]\n",
    "        # Upsample to input resolution for loss/metric compatibility\n",
    "        logits = F.interpolate(\n",
    "            logits, size=x.shape[2:], mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SegFormerWrapper(SEGFORMER_VARIANT, NUM_CLASSES).to(device)\n",
    "print(f\"Model: {SEGFORMER_VARIANT}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Functions\n",
    "\n",
    "Combined Dice + Focal loss — unchanged from the DeepLabV3+ baseline.  \n",
    "- Dice: optimizes region overlap, handles class imbalance.  \n",
    "- Focal: down-weights easy pixels, focuses on hard examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceFocalLoss(nn.Module):\n",
    "    \"\"\"Dice + Focal combined loss for multiclass segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mode=\"multiclass\" expects predictions [B, C, H, W] and targets [B, H, W] with class indices\n",
    "        self.dice = smp.losses.DiceLoss(mode=\"multiclass\", classes=NUM_CLASSES)\n",
    "        self.focal = smp.losses.FocalLoss(mode=\"multiclass\")\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.dice(pred, target) + self.focal(pred, target)\n",
    "\n",
    "\n",
    "criterion = DiceFocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics\n",
    "\n",
    "Multiclass IoU (Jaccard Index) — accumulated across batches, computed per-epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassIoU:\n",
    "    \"\"\"Accumulates predictions over batches, then computes mean IoU.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear accumulators at the start of each epoch.\"\"\"\n",
    "        self.intersection = np.zeros(self.num_classes)\n",
    "        self.union = np.zeros(self.num_classes)\n",
    "\n",
    "    def update(self, pred: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: raw logits [B, C, H, W]\n",
    "            target: class indices [B, H, W]\n",
    "        \"\"\"\n",
    "        # Convert logits to predicted class indices\n",
    "        pred_classes = pred.argmax(dim=1).cpu().numpy()  # [B, H, W]\n",
    "        target_np = target.cpu().numpy()                  # [B, H, W]\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            pred_mask = (pred_classes == cls)\n",
    "            target_mask = (target_np == cls)\n",
    "            self.intersection[cls] += np.logical_and(pred_mask, target_mask).sum()\n",
    "            self.union[cls] += np.logical_or(pred_mask, target_mask).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Return mean IoU across all classes. Ignores classes with zero union.\"\"\"\n",
    "        iou_per_class = np.zeros(self.num_classes)\n",
    "        for cls in range(self.num_classes):\n",
    "            if self.union[cls] > 0:\n",
    "                iou_per_class[cls] = self.intersection[cls] / self.union[cls]\n",
    "        mean_iou = iou_per_class.mean()\n",
    "        return mean_iou, iou_per_class\n",
    "\n",
    "\n",
    "metric = MulticlassIoU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop\n",
    "\n",
    "Modified from baseline: added **gradient clipping** (critical for transformer stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    print(\"Training Epoch\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping — critical for transformer stability.\n",
    "        # Without it, self-attention gradients can spike (especially\n",
    "        # in early epochs before warmup completes), causing NaN losses.\n",
    "        # This was NOT needed for the CNN baseline but IS needed here.\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # drop_last=True means we process fewer than len(dataset) samples\n",
    "    total_samples = len(loader) * loader.batch_size\n",
    "    return running_loss / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, metric, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    metric.reset()\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        metric.update(preds, masks)\n",
    "\n",
    "    # Same drop_last correction as training\n",
    "    total_samples = len(loader) * loader.batch_size\n",
    "    val_loss = running_loss / total_samples\n",
    "    mean_iou, per_class_iou = metric.compute()\n",
    "    return val_loss, mean_iou, per_class_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Execution\n",
    "\n",
    "Key differences from DeepLabV3+ training:\n",
    "- **Differential LR**: encoder (MiT-B2) at 6e-5, decode head at 6e-4\n",
    "- **AdamW** with weight_decay=0.01 (standard for transformers)\n",
    "- **Linear warmup** (3 epochs) + cosine decay — warmup is critical for transformers\n",
    "- **Scheduler actually stepped** (the baseline defined CosineAnnealingLR but never called .step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Differential learning rates for transformer fine-tuning ---\n",
    "# The pretrained MiT encoder should be updated gently to preserve\n",
    "# learned representations. The decode head (classifier reinitialized\n",
    "# for our 10 classes) needs faster adaptation.\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.model.segformer.parameters(), \"lr\": ENCODER_LR},\n",
    "    {\"params\": model.model.decode_head.parameters(), \"lr\": DECODER_LR},\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "# --- Warmup + cosine decay scheduler ---\n",
    "# Warmup is critical for transformers: self-attention layers are sensitive\n",
    "# to large gradient updates before the loss landscape stabilizes.\n",
    "# Linear warmup lets the model \"orient\" itself before full-speed optimization.\n",
    "def lr_lambda_fn(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return (epoch + 1) / WARMUP_EPOCHS  # linear: 0.33 -> 0.67 -> 1.0\n",
    "    # Cosine decay over remaining epochs\n",
    "    progress = (epoch - WARMUP_EPOCHS) / max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fn)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "best_miou = 0.0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    val_loss, val_miou, per_class_iou = validate(model, val_loader, criterion, metric, device)\n",
    "    scheduler.step()  # Step per epoch (was missing in baseline!)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_miou\"].append(val_miou)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val mIoU: {val_miou:.4f} | \"\n",
    "          f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "    # Print per-class IoU every 10 epochs for monitoring\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        for cls_idx, iou_val in enumerate(per_class_iou):\n",
    "            print(f\"  Class {CLASS_IDS[cls_idx]:>5d} (idx {cls_idx}): IoU = {iou_val:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        save_path = os.path.join(SAVE_DIR, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  -> Saved best model (mIoU={best_miou:.4f})\")\n",
    "\n",
    "# Save final checkpoint\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"final_model.pth\"))\n",
    "print(f\"\\nTraining complete. Best mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualization & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training curves ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss Curves\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history[\"val_miou\"], label=\"Val mIoU\", color=\"green\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"mIoU\")\n",
    "axes[1].set_title(\"Validation mIoU\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual inference on validation samples ---\n",
    "\n",
    "# Distinct colors for each class (RGB for matplotlib)\n",
    "CLASS_COLORS = [\n",
    "    (128,   0,   0),    # 100  - dark red\n",
    "    (  0, 128,   0),    # 200  - green\n",
    "    (  0,   0, 128),    # 300  - dark blue\n",
    "    (128, 128,   0),    # 500  - yellow-ish\n",
    "    (  0, 128, 128),    # 550  - cyan-ish\n",
    "    (128,   0, 128),    # 600  - magenta\n",
    "    (  0, 255,   0),    # 700  - bright green\n",
    "    (  0,   0, 255),    # 800  - blue\n",
    "    (255, 255,   0),    # 7100 - yellow\n",
    "    (  0, 255, 255),    # 10000- cyan\n",
    "]\n",
    "\n",
    "\n",
    "def colorize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert class-index mask to a colored RGB image.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cls_idx in range(NUM_CLASSES):\n",
    "        colored[mask == cls_idx] = CLASS_COLORS[cls_idx]\n",
    "    return colored\n",
    "\n",
    "\n",
    "def postprocess_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Morphological post-processing to clean up noisy predictions.\n",
    "    Applies per-class opening (remove small spurious pixels)\n",
    "    followed by closing (fill small holes).\n",
    "    \"\"\"\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    cleaned = np.zeros_like(mask)\n",
    "\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        binary = (mask == cls).astype(np.uint8)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned[binary == 1] = cls\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def remap_to_original(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert contiguous labels [0-9] back to original class IDs for submission.\"\"\"\n",
    "    out = np.zeros_like(mask, dtype=np.uint16)\n",
    "    for idx, original_id in enumerate(CLASS_IDS):\n",
    "        out[mask == idx] = original_id\n",
    "    return out\n",
    "\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best_model.pth\"), map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Inference transform (same as val — resize + normalize only)\n",
    "infer_transform = get_val_transforms(IMG_SIZE)\n",
    "\n",
    "# Pick a few validation samples\n",
    "val_image_dir = os.path.join(DATA_ROOT, \"val\", \"Color_Images\")\n",
    "val_mask_dir = os.path.join(DATA_ROOT, \"val\", \"segmentation\")\n",
    "val_images = sorted(os.listdir(val_image_dir))\n",
    "val_masks = sorted(os.listdir(val_mask_dir))\n",
    "\n",
    "NUM_SAMPLES = 6\n",
    "fig, axes = plt.subplots(NUM_SAMPLES, 3, figsize=(15, 5 * NUM_SAMPLES))\n",
    "if NUM_SAMPLES == 1:\n",
    "    axes = axes[np.newaxis, :]  # ensure 2D indexing\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # Load original image\n",
    "    img_path = os.path.join(val_image_dir, val_images[i])\n",
    "    image_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    original_h, original_w = image_rgb.shape[:2]\n",
    "\n",
    "    # Load ground truth mask\n",
    "    gt_path = os.path.join(val_mask_dir, val_masks[i])\n",
    "    gt_mask = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED)\n",
    "    if gt_mask.ndim == 3:\n",
    "        gt_mask = gt_mask[:, :, 0]\n",
    "    gt_mask = remap_mask(gt_mask).astype(np.uint8)\n",
    "\n",
    "    # Run inference\n",
    "    augmented = infer_transform(image=image_rgb)\n",
    "    input_tensor = augmented[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "        logits = model(input_tensor)\n",
    "\n",
    "    pred_mask = logits.argmax(dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    # Resize prediction back to original image dimensions\n",
    "    pred_mask = cv2.resize(pred_mask, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Plot: original | ground truth | prediction\n",
    "    axes[i, 0].imshow(image_rgb)\n",
    "    axes[i, 0].set_title(f\"Image: {val_images[i]}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(colorize_mask(gt_mask))\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(colorize_mask(pred_mask))\n",
    "    axes[i, 2].set_title(\"Prediction\")\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"inference_results.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Notes\n",
    "\n",
    "### Architecture change: DeepLabV3+ (ResNet50) -> SegFormer (MiT-B2)\n",
    "\n",
    "**Why SegFormer?**\n",
    "- Global context via self-attention at every encoder stage — critical for large uniform regions (sky, sand) that CNNs struggle with due to limited receptive fields.\n",
    "- Position-encoding-free design makes it robust to scale variations — the same rock face at 10m and 100m gets similar attention patterns.\n",
    "- Lower texture bias than CNNs — reduces overfitting to synthetic rendering artifacts.\n",
    "- Lightweight MLP decode head trades heavy ASPP computation for reliance on richer encoder features.\n",
    "\n",
    "### What changed vs baseline\n",
    "\n",
    "| Component | DeepLabV3+ Baseline | SegFormer Pipeline |\n",
    "|-----------|--------------------|---------|\n",
    "| Model | DeepLabV3+ / ResNet50 / SSL | SegFormer / MiT-B2 / ADE20K |\n",
    "| Encoder LR | 1e-5 | 6e-5 |\n",
    "| Decoder LR | 1e-4 | 6e-4 |\n",
    "| Head LR | 1e-3 | (included in decoder) |\n",
    "| Weight decay | 1e-4 | 0.01 |\n",
    "| Scheduler | CosineAnnealing (unused!) | Warmup + Cosine (stepped) |\n",
    "| Gradient clipping | None | max_norm=1.0 |\n",
    "| Epochs | 30 | 40 |\n",
    "| Color augmentation | Aggressive | Reduced |\n",
    "| Noise/blur prob | 0.4 | 0.2 |\n",
    "\n",
    "### What stayed the same\n",
    "- Dataset class and class ID remapping (unchanged)\n",
    "- Loss: Dice + Focal (unchanged)\n",
    "- Metric: Multiclass IoU (unchanged)\n",
    "- Input: 512x512 (unchanged)\n",
    "- Batch size: 4 (unchanged)\n",
    "- AMP / GradScaler (unchanged)\n",
    "- Visualization and post-processing (unchanged)\n",
    "\n",
    "### If you hit OOM\n",
    "1. Switch `SEGFORMER_VARIANT` to `nvidia/segformer-b1-finetuned-ade-512-512` (~14M params vs ~27M)\n",
    "2. Or reduce `BATCH_SIZE` to 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
