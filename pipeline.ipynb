{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Road Desert Semantic Segmentation Pipeline\n",
    "\n",
    "Multiclass semantic segmentation using DeepLabV3+ with EfficientNet-B0.  \n",
    "Consolidated from: `dataset.py`, `model.py`, `losses.py`, `metrics.py`, `train.py`, `infer.py`, `visualize.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodePlayground\\KrackHack\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths — adjust DATA_ROOT if your dataset lives elsewhere\n",
    "DATA_ROOT = \"dataset\"\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters — identical to the original train.py defaults\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-4\n",
    "IMG_SIZE = 256\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class ID Mapping\n",
    "\n",
    "Mask pixel values are non-contiguous IDs: `{100, 200, 300, 500, 550, 600, 700, 800, 7100, 10000}`.  \n",
    "We remap them to contiguous `[0–9]` via a lookup table for O(1) per-pixel mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original class IDs found in the masks -> contiguous labels [0-9]\n",
    "CLASS_IDS = [100, 200, 300, 500, 550, 600, 700, 800, 7100, 10000]\n",
    "NUM_CLASSES = len(CLASS_IDS)\n",
    "\n",
    "# Build a lookup table for fast remapping (max value is 10000, so table size is 10001)\n",
    "_REMAP_LUT = np.full(10001, 0, dtype=np.int64)  # default to 0 for any unexpected value\n",
    "for contiguous_label, original_id in enumerate(CLASS_IDS):\n",
    "    _REMAP_LUT[original_id] = contiguous_label\n",
    "\n",
    "\n",
    "def remap_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Remap raw mask pixel values to contiguous class indices [0, NUM_CLASSES-1].\"\"\"\n",
    "    return _REMAP_LUT[mask]\n",
    "\n",
    "\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(f\"ID mapping: {dict(zip(CLASS_IDS, range(NUM_CLASSES)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n",
    "\n",
    "Pairs images and masks by **sorted index** — filenames do NOT match between `Color_Images/` and `segmentation/`.  \n",
    "Masks are loaded as uint16 (values like 7100 and 10000 exceed uint8 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pairs images and masks by SORTED INDEX, not by filename.\n",
    "    This is required because filenames differ between the two folders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Sort both lists independently — alignment is by index\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), (\n",
    "            f\"Mismatch: {len(self.images)} images vs {len(self.masks)} masks\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image as RGB (OpenCV loads BGR by default)\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load mask as uint16 grayscale — critical for values > 255 like 7100, 10000\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # If mask was loaded as 3-channel, take first channel\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        # Remap to contiguous class IDs [0-9]\n",
    "        mask = remap_mask(mask).astype(np.int64)\n",
    "\n",
    "        # Apply augmentations — Albumentations treats mask as integer labels automatically\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]           # float32 tensor [3, H, W]\n",
    "            mask = augmented[\"mask\"]             # int64 tensor [H, W]\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss / segmentation losses\n",
    "        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size=256):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms(img_size=256):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SegmentationDataset(\n",
    "    image_dir=os.path.join(DATA_ROOT, \"train\", \"Color_Images\"),\n",
    "    mask_dir=os.path.join(DATA_ROOT, \"train\", \"segmentation\"),\n",
    "    transform=get_train_transforms(IMG_SIZE),\n",
    ")\n",
    "val_ds = SegmentationDataset(\n",
    "    image_dir=os.path.join(DATA_ROOT, \"val\", \"Color_Images\"),\n",
    "    mask_dir=os.path.join(DATA_ROOT, \"val\", \"segmentation\"),\n",
    "    transform=get_val_transforms(IMG_SIZE),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples | Val: {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "DeepLabV3+ with ImageNet-pretrained EfficientNet-B0 encoder via `segmentation-models-pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build DeepLabV3+ with ImageNet-pretrained EfficientNet-B0 backbone.\"\"\"\n",
    "    model = smp.DeepLabV3Plus(\n",
    "        encoder_name=\"efficientnet-b0\",\n",
    "        encoder_weights=\"imagenet\",     # pretrained features for faster convergence\n",
    "        in_channels=3,\n",
    "        classes=NUM_CLASSES,            # 10 semantic classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Functions\n",
    "\n",
    "Combined Dice + Focal loss.  \n",
    "- Dice: optimizes region overlap, handles class imbalance.  \n",
    "- Focal: down-weights easy pixels, focuses on hard examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceFocalLoss(nn.Module):\n",
    "    \"\"\"Dice + Focal combined loss for multiclass segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mode=\"multiclass\" expects predictions [B, C, H, W] and targets [B, H, W] with class indices\n",
    "        self.dice = smp.losses.DiceLoss(mode=\"multiclass\", classes=NUM_CLASSES)\n",
    "        self.focal = smp.losses.FocalLoss(mode=\"multiclass\")\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.dice(pred, target) + self.focal(pred, target)\n",
    "\n",
    "\n",
    "criterion = DiceFocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics\n",
    "\n",
    "Multiclass IoU (Jaccard Index) — accumulated across batches, computed per-epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassIoU:\n",
    "    \"\"\"Accumulates predictions over batches, then computes mean IoU.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear accumulators at the start of each epoch.\"\"\"\n",
    "        self.intersection = np.zeros(self.num_classes)\n",
    "        self.union = np.zeros(self.num_classes)\n",
    "\n",
    "    def update(self, pred: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: raw logits [B, C, H, W]\n",
    "            target: class indices [B, H, W]\n",
    "        \"\"\"\n",
    "        # Convert logits to predicted class indices\n",
    "        pred_classes = pred.argmax(dim=1).cpu().numpy()  # [B, H, W]\n",
    "        target_np = target.cpu().numpy()                  # [B, H, W]\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            pred_mask = (pred_classes == cls)\n",
    "            target_mask = (target_np == cls)\n",
    "            self.intersection[cls] += np.logical_and(pred_mask, target_mask).sum()\n",
    "            self.union[cls] += np.logical_or(pred_mask, target_mask).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Return mean IoU across all classes. Ignores classes with zero union.\"\"\"\n",
    "        iou_per_class = np.zeros(self.num_classes)\n",
    "        for cls in range(self.num_classes):\n",
    "            if self.union[cls] > 0:\n",
    "                iou_per_class[cls] = self.intersection[cls] / self.union[cls]\n",
    "        mean_iou = iou_per_class.mean()\n",
    "        return mean_iou, iou_per_class\n",
    "\n",
    "\n",
    "metric = MulticlassIoU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, metric, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    metric.reset()\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        metric.update(preds, masks)\n",
    "\n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    mean_iou, per_class_iou = metric.compute()\n",
    "    return val_loss, mean_iou, per_class_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "best_miou = 0.0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    val_loss, val_miou, per_class_iou = validate(model, val_loader, criterion, metric, device)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_miou\"].append(val_miou)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val mIoU: {val_miou:.4f}\")\n",
    "\n",
    "    # Print per-class IoU every 10 epochs for monitoring\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        for cls_idx, iou_val in enumerate(per_class_iou):\n",
    "            print(f\"  Class {CLASS_IDS[cls_idx]:>5d} (idx {cls_idx}): IoU = {iou_val:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        save_path = os.path.join(SAVE_DIR, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  -> Saved best model (mIoU={best_miou:.4f})\")\n",
    "\n",
    "# Save final checkpoint\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"final_model.pth\"))\n",
    "print(f\"\\nTraining complete. Best mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualization & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training curves ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss Curves\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history[\"val_miou\"], label=\"Val mIoU\", color=\"green\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"mIoU\")\n",
    "axes[1].set_title(\"Validation mIoU\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual inference on validation samples ---\n",
    "\n",
    "# Distinct colors for each class (RGB for matplotlib)\n",
    "CLASS_COLORS = [\n",
    "    (128,   0,   0),    # 100  - dark red\n",
    "    (  0, 128,   0),    # 200  - green\n",
    "    (  0,   0, 128),    # 300  - dark blue\n",
    "    (128, 128,   0),    # 500  - yellow-ish\n",
    "    (  0, 128, 128),    # 550  - cyan-ish\n",
    "    (128,   0, 128),    # 600  - magenta\n",
    "    (  0, 255,   0),    # 700  - bright green\n",
    "    (  0,   0, 255),    # 800  - blue\n",
    "    (255, 255,   0),    # 7100 - yellow\n",
    "    (  0, 255, 255),    # 10000- cyan\n",
    "]\n",
    "\n",
    "\n",
    "def colorize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert class-index mask to a colored RGB image.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cls_idx in range(NUM_CLASSES):\n",
    "        colored[mask == cls_idx] = CLASS_COLORS[cls_idx]\n",
    "    return colored\n",
    "\n",
    "\n",
    "def postprocess_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Morphological post-processing to clean up noisy predictions.\n",
    "    Applies per-class opening (remove small spurious pixels)\n",
    "    followed by closing (fill small holes).\n",
    "    \"\"\"\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    cleaned = np.zeros_like(mask)\n",
    "\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        binary = (mask == cls).astype(np.uint8)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned[binary == 1] = cls\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def remap_to_original(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert contiguous labels [0-9] back to original class IDs for submission.\"\"\"\n",
    "    out = np.zeros_like(mask, dtype=np.uint16)\n",
    "    for idx, original_id in enumerate(CLASS_IDS):\n",
    "        out[mask == idx] = original_id\n",
    "    return out\n",
    "\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best_model.pth\"), map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Inference transform (same as val — resize + normalize only)\n",
    "infer_transform = get_val_transforms(IMG_SIZE)\n",
    "\n",
    "# Pick a few validation samples\n",
    "val_image_dir = os.path.join(DATA_ROOT, \"val\", \"Color_Images\")\n",
    "val_mask_dir = os.path.join(DATA_ROOT, \"val\", \"segmentation\")\n",
    "val_images = sorted(os.listdir(val_image_dir))\n",
    "val_masks = sorted(os.listdir(val_mask_dir))\n",
    "\n",
    "NUM_SAMPLES = 6\n",
    "fig, axes = plt.subplots(NUM_SAMPLES, 3, figsize=(15, 5 * NUM_SAMPLES))\n",
    "if NUM_SAMPLES == 1:\n",
    "    axes = axes[np.newaxis, :]  # ensure 2D indexing\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # Load original image\n",
    "    img_path = os.path.join(val_image_dir, val_images[i])\n",
    "    image_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    original_h, original_w = image_rgb.shape[:2]\n",
    "\n",
    "    # Load ground truth mask\n",
    "    gt_path = os.path.join(val_mask_dir, val_masks[i])\n",
    "    gt_mask = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED)\n",
    "    if gt_mask.ndim == 3:\n",
    "        gt_mask = gt_mask[:, :, 0]\n",
    "    gt_mask = remap_mask(gt_mask).astype(np.uint8)\n",
    "\n",
    "    # Run inference\n",
    "    augmented = infer_transform(image=image_rgb)\n",
    "    input_tensor = augmented[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "        logits = model(input_tensor)\n",
    "\n",
    "    pred_mask = logits.argmax(dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    # Resize prediction back to original image dimensions\n",
    "    pred_mask = cv2.resize(pred_mask, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Plot: original | ground truth | prediction\n",
    "    axes[i, 0].imshow(image_rgb)\n",
    "    axes[i, 0].set_title(f\"Image: {val_images[i]}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(colorize_mask(gt_mask))\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(colorize_mask(pred_mask))\n",
    "    axes[i, 2].set_title(\"Prediction\")\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"inference_results.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Notes\n",
    "\n",
    "**Dataset**\n",
    "- 2857 train / 317 val image-mask pairs\n",
    "- Masks are uint16 PNGs — loaded with `cv2.IMREAD_UNCHANGED` to preserve values > 255\n",
    "- Images and masks paired by sorted index, NOT by filename\n",
    "\n",
    "**Architecture**\n",
    "- DeepLabV3+ with EfficientNet-B0 encoder (ImageNet pretrained)\n",
    "- 10 semantic classes remapped from `{100, 200, 300, 500, 550, 600, 700, 800, 7100, 10000}` to `[0-9]`\n",
    "\n",
    "**Training**\n",
    "- Dice + Focal combined loss\n",
    "- Adam optimizer, lr=1e-4\n",
    "- Mixed precision (AMP) for GPU acceleration\n",
    "- Best model saved by validation mIoU\n",
    "\n",
    "**Augmentations** (train only)\n",
    "- HorizontalFlip, RandomBrightnessContrast, ColorJitter, GaussNoise\n",
    "- Both train and val: Resize to 256x256, ImageNet normalization\n",
    "\n",
    "**Post-processing**\n",
    "- Morphological opening + closing to clean noisy predictions\n",
    "- `remap_to_original()` converts contiguous labels back to original class IDs for submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
